\documentclass[10pt]{article}
\usepackage{../../../local}
\urlstyle{same}
\linespread{1.1}
\begin{document}
\section{Concentration Inequalities}
\begin{itemize}
	\item Markov's inequality is:
		 \[
		P(X \ge k) \le \frac{E(X)}{k}
		\] 
		The proof comes from the expectation formula:
		\[
			E(X) = \sum_{\text{all \( a \)}} a P(X = a) \ge \sum_{ a >=k} a P(X = a) \ge \sum_{a \ge k} k
			P(X = a) = k \sum_{a \ge  k} P(X = a) = k P(X \ge k)
		\] 
		From here, we can conclude that 
		\[
		P(X \ge k) \le \frac{E(X)}{k}
		\] 
		this inequality is good, but its a fairly weak upper bound. Note also that Markov's has the inherent 
		downside that it requires your random variable \( X \) to be nonnegative, which isn't always the case. 
	\item Chebyshev's inequality is:
		\[
		P(|X - \mu| \ge c ) \le \frac{Var(X)}{c^2}
		\] 
		this formula gives a much tighter bound for \( X \), but it requires a bit more work to get there. 
		Markov's is nice in that if you're given a value of \( k \) to work with, then you know to just plug 
		\( k \) into the formula. However, with Chebyshev's, you need to do a bit of work in choosing \( \mu \) 
		and \( c \) in order to get the bound you want.  

		Chebyshev's inequality has the downside that it gives you a two-sided bound, instead of the one-sided bound 
		that is guaranteed by Markov's. This is because of the \( |X - \mu| \ge c \) term. 
	\item I also think that with Chebyshev's, the two-tailed sum will end up giving you some overhead (in the 
		two-sided limit), but that's kind of the price you have to pay.  
\end{itemize}
\section{LLN}
\begin{itemize}
	\item LLN basically is an application of Chebyshev's, which basically says that as \( N \) grows large, then the 
		sample mean approaches the true mean of the distribution. This is what it means when you flip a coin 10000 
		times and your sample mean (the flips) roughly matches the true mean of \( \frac{1}{2} \).  

		This is a result of the fact that the variance of the mean for a random variable scales with \( \frac{1}{n} \), 
		so as \( n \) increases, the variance in the mean grows smaller.  
\end{itemize}
\section{Continuous Probability}
\begin{itemize}
	\item We went over the continuous analogues last week, so I won't bother writing them here. 
\end{itemize}

\end{document}
