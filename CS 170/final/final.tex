\documentclass[10pt]{article}
\usepackage{../../local}
\urlstyle{same}

\newcommand{\classcode}{CS170}
\newcommand{\classname}{Efficient Algorithms and Intractable Problems}
\renewcommand{\maketitle}{%
\hrule height4pt
\large{Eric Du \hfill \classcode}
\newline
\large{Final Review} \Large{\hfill \classname \hfill} \large{\today}
\hrule height4pt \vskip .7em
\small{Header styling inspired by CS 70: \url{https://www.eecs70.org/}}
\normalsize
}
\linespread{1.1}

\newcommand{\pre}{\mathrm{pre}}
\newcommand{\prev}{\mathrm{prev}}
\newcommand{\post}{\mathrm{post}}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\cost}{\mathrm{cost}}
\newcommand{\size}{\mathrm{size}}
\newcommand{\capacity}{\mathrm{capacity}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\question}[1]{\textcolor{red}{#1}}
\newcommand{\answer}[1]{\textcolor{green!80!black!}{#1}}
\renewcommand{\comment}[1]{\textcolor{blue!50}{#1}}
\begin{document}
	\maketitle
	\section{Graphs}

	\subsection{DFS}
	\begin{itemize}
		\item Explore unvisited nodes with some tiebreaking algorithm, and add unvisited ones to a stack.  
			Classifies edges, so for an edge \( (u, v) \) (i.e. \( u \to v \) ):
			\begin{itemize}
				\item Forward edge: edge explored in DFS tree (same as a tree edge mathematically): 
				\( \pre(u) < \pre(v) < \post(v) < \post(u) \). 
				\item Back edge: an child to an ancestor in DFS tree: 
					\( \pre(v) < \pre(u) < \post(u) < \post(v) \). 
				\item Cross Edge: neither a child or an ancestor (unrelated nodes)
					\( \pre(v) < \post(v) < \pre(u) < \post(u) \)

					\question{Which one of these requires that the edge \( (u, v) \) exists in the graph?}
			\end{itemize}
		\item Runtime: \( O(n + m) = O(|V| + |E|) \)
	\end{itemize}
	\subsection{DAGs}
	\begin{itemize}
		\item Directed acyclic graphs: directed graphs with no cycles basically.
		\item There are \( n \choose 2 \) edges, beause every pair of vertices may only be chosen once. 
		\item DAGs are special because they can always can be \textbf{topologically sorted:} arrange 
			the vertices left to right in such a way that outgoing edges only go to the right.  
		\item Topological sort: sort based on decreasing postorder (from left to right). The smallest 
			postorder must be a sink (all edges lead to \( v \) ), and largest postorder must be a source 
			(all edges lead away from \( v \)). 

			\question{Do I need to study the proof of this?}
	\end{itemize}
	\subsection{SCCs}
	\begin{itemize}
		\item Two vertices are strongly connected if there exists a path from \( v \to u \) and also a 
			path from \( u \to v \). This requires the graph to have a cycle (by definition)
		\item The \textit{meta graph} (graph of SCCs) must always be a DAG. This is because had there been 
			a cycle between two SCCs, this implies that the two SCCs should be combined together to form a new 
			SCC. 
		\item Kosaraju's algorithm: reverse all edges in graph, then run DFS on the graph in decreasing postorder.
			Each end of the explore call will find a new SCC every time.  
	\end{itemize}
	\subsection{Path finding algorithms}
	\begin{itemize}
		\item Given a graph \( G = (V, E) \) and edge weights \( e_i \), what is the shortest path from 
			a node \( u \) to another \( v \)?
		\item BFS: A way to find the shortest path between \( u \) and \( v \) given that all edge weights are 
			1. \( O(m + n) = O(|V| + |E|) \)
		\item Dijkstra's: A way to find the shortest \( u \to v \) given that all edge weights are positive.
			\( O(n \log n + m) = O(|V| \log|V| + |E|) \). 
			\comment{This heavily depends on the implementation of Dijkstra's, this runtime is only possible 
			given we use a fibonacci heap.}
		\item Bellman-Ford: A way to find the shortest \( u \to v \) path with no restrictions on 
			edge weights. \( O(nm) = O(|V| |E|) \). 
			\comment{The only restriction is that we can't have negative weight cycles, since then the notion 
			of a shortest path is broken.}
	\end{itemize}
	\section{Greedy Algorithms}
	\begin{itemize}
		\item An algorithm that during runtime, makes the most naively optimal choice every time (i.e. greedy). 
		\item We generally use induction to prove that greedy is always optimal.
		\item Class scheduling: given a set of classes, what is the maximum we can fit in a room? We solve this 
			by adding based on earliest end time.   
	\end{itemize}
	\subsection{Huffman Coding}
	\begin{itemize}
		\item Given a text with letter frequencies, what's the minimum encoding we can encode with such that 
			a unique message is recoverable?
		\item Generate a tree based on frequencies: the most frequent letter should use the least bits, and 
			so on. Greedy solution here is in fact optimal!
		\item The cost function can be calculated as:
			\[
			\cost = \sum_i f_i \cdot d(i)
			\] 
			where \( d(i) \) represents the depth at which we find character \( i \).  
		\item Specifically, it creates a priority queue based on frequency and merges least frequent 
			nodes, adds them together and puts them back in the queue. 
		\item Runtime: \( O(n \log n) \) when utilizing a binary heap. 
	\end{itemize}
	\subsection{Horn-SAT}
	\begin{itemize}
		\item Satisfiability problem: given a set of clauses \( x_i \lor x_j \dots \lor x_k \) and a list 
			of implications \( (\text{stuff}) \implies x_k \), we want to know whether this is satisfiable.
		\item Greedy solution: for every clause of the form \( (\text{stuff}) \implies x_k \), set 
			\( x_k \) to true. Do this for every clause. Then, check if all pure negative clauses 
			\( (x_i \lor x_j \cdots) \) are satisfied. If yes, then return true, otherwise the instance is 
			not satisfiable. 
		\item The reason this outputs an optimal solution is the fact that in order for the instance to be satisfiable, we 
			do the "minimum" we can possibly do to satisfy the clauses, or in other words setting \( x_k \) to true for every 
			implication we find. Because at every step we're doing the \textit{minimum possible}, this ends up being 
			the best optimal solution. 
	\end{itemize}

	\subsection{MSTs}
	A minimum spanning tree is basically a tree of a graph \( G = (V, E) \) that contains all vertices \( v \in V \), and 
	has minimum sum of edge weights. There could be multiple MSTs, given that the edge weights sum up to the same thing (for 
	instance, consider a complete graph \( G \) whose edge weights are all 1. There are two main MST-finding algorithms 
	that we need to know: Prim's and Kruskal's algorithm:
	\begin{itemize}
		\item Kruskal's Algorithm: start with an empty set, and always add the minimum weight edge \( e \) such that 
			adding that edge doesn't create a cycle. Repeat this until all vertices are covered, and we have an MST. 
		\item Prim's Algorithm: start with a set \( S = \{ v \}  \)  and consider the set \( T = V \setminus S \), of all the 
			edges from \( S \) to \( T \), consecutively add the minimum weight edge. 
	\end{itemize}
	The most important thing about MSTs is the \textbf{cut property:} Given that we have a set of edges \( X \) that's already 
	in the MST, then the lightest weight edge that connects \( X \) to \( V \setminus X \) is part of the MST as well. This 
	implies that we can create a meta algorithm:

	\begin{tcolorbox}
		\begin{itemize}
			\item Start with an empty set, and pick \( S \subseteq V \) such that \( X \) has no edges from \( X \) to 
				\( V \setminus X \). 
			\item Add the lighest weight edge from \( X  \) to \( V \setminus X \). 
		\end{itemize}
	\end{tcolorbox}
	Any algorithm that follows this meta algorithm is guaranteed to find an MST. The thing that algorithms do differently is 
	how they pick \( S \), and then correspondingly how they pick \( e \). For Prim's, it's fairly obvious how this is chosen, and 
	for Kruskal's, the condition that the added edge \( e \) doesn't form a cycle is specifically what guarantees
	that the edge crosses from \( X \) to \( S \setminus X \). 

	\subsection{Set Cover}
	Given a set of sets \( S = \{S_1, S_2, \dots, S_n\}  \), what is the minimum number of sets that we can choose such that
	the union of the selected sets covers the entire vertex set \( V \)? Here, one might want to consider a greedy solution, but this 
	doesn't give an optimal solution! Instead, what we can show is that if the optimal solution uses \( k \) sets, then the 
	greedy solution uses \( k \ln n\) sets. 

	To prove this, we prove it recursively, by letting \( n_t \) be the number of elements not covered by the greedy algorithm 
	after \( t \) time steps. Then, if we want it to find \( k \ln n\) steps at time \( t \), then this means that 
	we want \( n_t < 1 \) (since that implies that there aren't any more remaining vertices). To show is is true, note that 
	\( n_1 \le  n_0 - \frac{n_0}{k} \). This is because if the optimal solution uses \( k \) sets, then \textit{on average} 
	the greedy solution uses a set size of \( \frac{n}{k} \), so there must be a set with more than \( \frac{n}{k} \) elements. 
	This naturally extends to the next \( t \) iterations. Asymptotically, this approaches the term \( \ln n \), hence 
	we're done. 


	\section{Dynamic Programming}

	Dynamic programming is essentially the philosophy of taking a large problem, and break it down recursively into smaller 
	and smaller problems. This differs from divide and conquer, where the steps required to compute any given step is only 
	dependent on the smaller two subproblems. In DP, this is not true. 

	Consider computing \( F(n) \), the \( n \)-th Fibonacci number. One way we may achieve that is to use the formula 
	\( F(n) = F(n-1) + F(n - 2) \) repeatedly until we get down to the definitions \( F(0) = 0, F(1) = 1 \). This solution would 
	take exponential time (\( O(2^{n}) \) in fact), and takes way too long. However, notice the repeated calculation here! 
	Whenever we try to calculate \( F(n) \), \( F(1) \) is accessed a total of \( 2^{n}  \) times, so can we optimize this 
	in some way? 

	The key is to use \textbf{memoization}: this is a strategy where we store the results of previous subproblems into memory, and
	access these memory locations in order to prevent us from having to compute these subproblems again. Generally, we will do 
	our DP problems from the bottom up, meaning that we won't deal with recursion, and start solving from the smallest 
	subproblem and work our way up. 

	\subsection{Shortest path with \( k \) edges}
	
\end{document}
