\section{Randomized Algorithms} 
\begin{itemize}
	\item Described as an algorithm which uses random bits to solve problems. Generally, this is done 
		using some function called \( \textsc{RandomInt}(a, b) \) which returns a random integer from 
		the interval \( (a, b) \).
	\item Generally, algorithms will output a \textit{pseudorandom} number (random enough for our purposes) 
		But when we model randomness mathematically, we refer to a genuinely random number.  
	\item Here, algorithms will be allowed to fail with some probability (say 5\%).
	\item Oftentimes, randomized algorithms lead to much more elegant solutions when compared to deterministic 
		algorithms. They can also sometimes be much faster than deterministic ones!
\end{itemize}
\subsection{Background: Integer Factorization}
\begin{itemize}
	\item This is not a problem we know how to solve using a randomized algorithm. Given a number \( N \), 
		we want to find its prime factorization \( N = p_1p_2\cdots p_k \). 
	\item Naively, we could just test every single number from 1 to \( \sqrt{N}  \), but this takes 
		\( O(\sqrt{N} ) \) time (if \( N \sim 10^{500} \), then our algorithm would take longer than the number 
		of atoms in the universe to compute.)
	\item The best algorithm we know today is called a \textit{General Number Field Sieve}, which factors 
		an \( n \) bit number in time \( C^{n^{1 / 3}\log(n)^{2 / 3}} \). (This \( C \) is not very large, but 
		still not small.) 
	\item This is a very important problem! RSA (the company) literally puts out numbers on the internet with 
		cash prizes attached to them if you can factor them. RSA-250 is a 250-digit number, which was 
		recently factored in 2020. RSA-290 has a \$75,000 cash prize attached to it.
	\item \textit{Aside:} this is a problem that is easily solved by quantum computers.  
\end{itemize}
\subsection{Primality Testing}
\begin{itemize}
	\item Given a number \( N \), our only goal is to figure out whether \( N \) is prime or composite. 
	\item This is very similar to the prime factorization problem! We \textit{could} take \( N \) and factor it, 
		but this is obviously very hard. But we can solve this relatively efficiently using randomness!
	\item We use another property of prime numbers: Fermat's little theorem! It says that if \( N \) is prime, 
		then 
		\[
			a^{N-1} \equiv 1 \pmod{N}
		\] 
		for all \( a \in \{1, 2, \dots, N-1\}  \).
	\item So we define a test called \( \textsc{FermatTest}(N) \). It picks a value \( a \in \{1, 2, \dots, 
		N - 1\}  \) uniformly at random. If \( a^{N} \equiv 1 \pmod N \), then we output "prime", otherwise 
		we output "composite". 
		\begin{itemize}
			\item This algorithm will always output "prime" for prime \( N \). For values that are coprime with 
				\( N \), it's harder to see whether they pass \( \textsc{FermatTest} \), and ones that aren't 
				are certainly going to fail \( \textsc{FermatTest} \).
			\item Becuase this test relies pretty heavily on the fact that \( a \) is coprime, our test is only 
				as good as the number of coprime \( a \) that we get. 
			\item \textit{Aside:} There do in fact exist are \textit{composite} numbers that satisfy
				\( a^{N-1} \equiv 1 \pmod N \) for all \( a \) coprime
		to \( N \)! In other words, these numbers will always pass \textsc{FermatTest}. These are called 
		\textit{Carmichael numbers}, but for the purposes of this class, we will pretend like these don't exist.
		\end{itemize}
	\item \textbf{Theorem:} Suppose \( N \) is composite and not carmichael. Then, we have 
		\( P(\textsc{FermatTest}(N) = \text{composite}) \ge 1 / 2 \). 
		
		\textit{Proof:} If the number is not Carmichael, this implies the existence of some coprime \( b \)
		such that \( b^{N - 1} \not \equiv 1 \pmod N \), and thus there is a single \( b \) that fails
		\textsc{FermatTest}. 
		
		\textit{Subclaim:} Suppose some value \( a \) passes \textsc{FermatTest}. Then, \( ab \pmod N\)  will fail
		\textsc{FermatTest}. 

		\textit{Proof:} This is because \( ab \) is no longer coprime to \( N \), so this gives us a 
		1-1 correspondence between a value that passes \textsc{FermatTest} and one that doesn't. More 
		rigorously, we have:
		\begin{align*}
			(ab)^{N - 1} & \equiv a^{N - 1} \cdot b^{N - 1}\pmod N\\
						 &\equiv b^{N - 1} \pmod N\\
						 &\not \equiv 1 \pmod N
		\end{align*}
		where we use the fact that \( a^{N - 1} \equiv 1 \pmod N \). Then, the one-to-one correspondence follows.
		Then, since there are values that \textit{just fail} \textsc{FermatTest}, then this means that 
		there are more values that fail than pass. Hence, \( P(\text{fail}) \ge  1 / 2 \), as desired.   
	\item Now, let's repeat this algorithm \( k \) times. On any given test, we know that if \textsc{FermatTest}
		returns "composite" then \( N \) is definitely composite, so the only case where we get a fail is if 
		\( N \) is composite but \textsc{FermatTest} outputs "prime". This occurs with probability 
		\[
		P(\text{\( N \) composite but outputs prime}) = 1 - \frac{1}{2^{k}}
		\] 
		This means that when our test outputs "prime", there is a \( 1 - \frac{1}{2^{k}} \) chance of it being 
		a false positive. 
	\item We can add another check to detect Carmichael numbers -- we won't really delve into this here. This 
		gives the Miller-Rabin primality test, developed in 1976. 
	\item Since then, there have been improvements: the AKS Primality test (2002) gave a deterministic 
		polynomial time \( O(n^{12}) \) algorithm for primality testing.   
\end{itemize}
\subsection{Randomized Complexity Classes}
\begin{itemize}
	\item Some problems have both efficient randomized and deterministic algorithms, but there are others 
		that only have efficient randomized algorithms. (e.g. polynomial identity testing) 
	\item This implies the existence of two possible worlds:
		\begin{enumerate}[label=\arabic*.]
			\item Every efficient randomized algorithm has a corresponding deterministic solution. (P)
			\item Some problems only have efficient randomized algorithms. (BPP)
		\end{enumerate}
	\item The question of which world we live in is called the P vs. BPP problem. P is the class of efficiently 
		deterministic problems (same P as before), and BPP is the class of efficient, randomized algorithms. We 
		think that these complexity classes are equal. 
\end{itemize}
\subsection{Minimum Cut} 
\begin{itemize}
	\item Consider a (unweighted, undirected) graph \( G = (V, E) \), and we want to find the minimum cut of this graph. Here, 
		the heuristic we're using to quantify the size of the cut is the number of edges it crosses through.
	\item Recall from our max-flow problem, we found the minimum \( s-t \) cut via our flow algorithm. It turns out that 
		 we can use the flow algorithm to solve this deterministically via a reduction, but today we'll talk about another 
		 algorithm: Karger's algorithm. 
	 \item For every \( i = 1, \dots, n- 1 \), it will:
		 \begin{enumerate}[label=\arabic*)]
		 	\item Pick a uniformly random edge \( e \), and "contract" the edge.
			\item It will return the cut specified by the remaining two supervertices.  
		 \end{enumerate}
	 \item The way to contract an edge is to combine the two vertices connected by \( e \), and combine them into one supervertex.
		 It will preserve all the possible edges, which means that the graph doesn't have to remain simple in this process.  
		 Then, once only two vertices are remaining, then that's the cut that we output. Notice that because we don't delete any
		 edges in this process, then the size of the "supercut" is equal to the size of the cut in the original graph. 
	 \item This won't return the minimum cut, but it will return the minimum cut with fairly good probability.  
\end{itemize}
\subsubsection{Intuition}
\begin{itemize}
	\item So suppose we have a graph with a bunch of edges, then with one singular edge outside, like this:
		% add a figure here
	\item If Karger's algorithm is to find the minimum cut, then it should never contract edge \( e \). Basically, it leverages
		 the fact that becuase there are more edges outside of the minimum cut, then the algorithm shouldn't select the edges 
		 along the minimum cut. 
	 \item \textbf{Theorem:} Let  \( C = (S, \overline S) \) be a minimum cut of size \( k \). Then, the probability that 
		 Karger's algorithm outputs the cut \( C \) is given by:
		 \[
			 P(\text{Karger's outputs \( C \)}) \ge \frac{1}{{n \choose 2}} = \frac{2}{n(n-1)}
		 \] 
		 While this is indeed bad for a single iteration, it just means that in expectation we just need to repeat this 
		 \( n^2 \) times to get the minimum cut!
	 \item \textit{Proof:} First, we state four facts:
		 \begin{itemize}
		 	\item Let \( G_i \) be the state of the graph \( G \) at the \textit{start of the} \( i \)-th iteration. Then, because 
		 \( C \) is the minimum cut with size \( k \), then we know that the minimum cut of \( G_i \ge k \). 
			\item Further, the number of vertices in \( G_i  \) is \( n - (i - 1) = n-i+1\). 
			\item The degree of each vertex in \( G_i \ge  k \), because of the fact that the minimum cut is \( k \). Had 
				the degree been less than \( k \), then the minimum cut would be less than \( k \), since the min-cut 
				would just be to cut that vertex with degree less than \( k \). 
			\item The number of edges in \( G \) can be calculated as:
				\begin{align*}
					N &= \frac{1}{2}\sum_{v \in G_i} \deg(v)\\
					  &\ge \frac{1}{2}\sum_{v \in G_i} k\\
					  &= \frac{1}{2}k |G_i| \\
					  &= \frac{1}{2}k(n-i+1) 
				\end{align*}
		 \end{itemize}
		 Now we complete the proof. Essentially, we want to compute the probability that the algorithm never contracts a cut 
		 in \( C \). Suppose at step \( G_i \), we haven't contracted any edges in \( C \). Then, the probability 
		 that we don't contract an edge in \( C \) is:
		 \[
		 	P(\text{don't contract an edge in \( C \)} = 1-P(\text{contract an edge in \( C \)}) 
			= 1 - \frac{k}{\frac{1}{2}k (n - i + 1)}
		\]
		Thus, the probability that we never contract an edge in \( C \) is the probability that this event occurs on every 
		iteration \( i \), at the step \( G_{i - k} \) :
		\[
		P(\text{never contract edge in  \( C \)}) \ge  \left( 1 - \frac{2}{n} \right) \left( 1 - \frac{2}{n-1} \right) \dots 
		= \frac{2}{n(n-1)}
		\] 
		So therefore, the probability that we succeed is lower bounded by \( \frac{2}{n^2} \).
\end{itemize}