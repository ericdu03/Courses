\section{Streaming II}
\begin{itemize}
	\item Recall the way we computed the number of distinct elements 
		using a random hash function \( h: \{1, \dots, N\} \to [0, 1] \). 
	\item There were problems with this approach! There were two:
		\begin{itemize}
			\item Computers can't store arbitrarily real numbers

				\textit{Solution:} Pick a hash function that maps \( h: \{1, \dots, R\}  \) instead of 
				\( \{1, \dots, N\}  \) so that \( h(i) / R \) is basically a random number between 0 and 1. This 
				is fairly easy to implement.
			\item If the hash function is uniformly random, then we would require \( N \log R \) bits to 
				store this -- this takes up a lot of memory, especially as \( R \) grows large! 

				\textit{Solution:} Make the hash function \( h \) ``pseudorandom''. We will define 
				a \textit{hash family} \( \mathcal H = \{h_1, \dots, h_m\}  \), and we will choose from this 
				family for our hash function. We will use \( h \sim \mathcal H \) to denote choosing a random 
				\( h_i \). This approach only requires \( \log m \) bits to store, so this is much better
				than our earlier implementation. 

				\comment{Each of \( h_i \) are basically constant functions, we've changed the randomness
				of the hash function to \textit{choosing} the hash function itself.}

				\question{So are the \( h_i \) just random bits, then how are we guaranteeing that we can still 
				get the number of distinct elements?}

				\answer{Just in the same way the hash function expects a spacing of \( \frac{1}{k+1} \), 
				we expect that the randomly generated hash family is also \( \frac{1}{k+1} \).} 
		\end{itemize}
\end{itemize}
\subsection{Pairwise Independence}
\begin{itemize}
	\item This is the way we're going to define randomness.
	\item \textbf{Definition:} A hash family \( \mathcal H = \{h_1, \dots, h_m : \{1, \dots, N\} \to 
		\{1, \dots, R\}	\} \) is \textit{pairwise independent} if:
		\begin{itemize}
			\item For all \( x \neq y \in \{1, \dots, N\}  \) and \( i, j \in \{1, \dots, R\}  \), then 
				\[
					\underset{h\sim \mathcal H}{\mathrm{Pr}}\left[ h(x) = i \text{ and } h(y) = j \right]  
					= \frac{1}{R^2}
				\] 
				where Pr denotes the probability. In other words, it means that our \( i, j \) look like 
				two independent draws from \( \{1, \dots, R\}  \). 
		\end{itemize}
	\item If this is true, it also implies that \( \underset{h \sim \mathcal H}{\mathrm{Pr}}[h(x) = i] =\frac{1}{R} \) for all \( x \) and \( i \).
\end{itemize}
\subsubsection{Generating Pairwise Independence}
\begin{itemize}
	\item Our approach will utilize modular arithmetic, and to make things easy we will do modular arithmetic 
		using a prime \( p \). 
	\item Then, for each \( a, b \in \mathbb{Z}_p = \{0, 1, \dots, p-1\}\), we will define \( h_{a, b}: 
		\mathbb Z_p \to 
		\mathbb Z_p\) such that \( h_{a, b}(x) = ax + b \pmod p\). 
	\item This makes \( \mathcal H = \{h_{a, b}\} _{a, b \in \mathbb Z_p} \) is pairwise independent. Note 
		also that there are \( p^2 \) hash functions, so \( \mathcal H \) takes up \( O(p) \) space.   

		\comment{This is very close to what you'd do with the streaming algorithm, but not exactly.}

		\textit{Proof of Independence (simplified):} Let  \( x, y, i, j \in \mathbb Z_p \) 
		such that \( x \neq y \). We want to show that 
		\[
			\underset{a, b}{\mathrm{Pr}}[ax + b = i \text{ and } ay + b = j] = \frac{1}{p^2}
		\] 
		We will do it for \( x = 0, y = 1 \) and the general case is left as an 
		exercise. In this case, we'd want to prove:
		\[
			\underset{a, b}{\mathrm{Pr}}[b = i \text{ and } a+ b = j] = \frac{1}{p^2}
		\] 
		But given this construction, we know that \( b \) and \( a + b \) is a random pair in \( \mathbb Z_p^2 \),
		so the probability is indeed \( \frac{1}{p^2} \). Note that this is not 3-wise independent: given 
		\( x =0, y = 1, z = 2 \), then \( h(z) = 2a + b = 2(a + b) - b = 2h(1) - h(0) \). Since there's 
		a way to construct \( h(z)  \), this is not independent.
\end{itemize}
\subsection{Modified Distinct Elements}
\begin{itemize}
	\item Instead of throwing our value into a hash function, we will pick a pairwise independent hash 
		function \( h: \{1, \dots, N\}  \to [0, 1] \). 
	\item We compute the \( t \)-th smallest value of \( \{h(s_1), \dots, h(s_L)\}  \), which would be 
		the \( t \)-th smallest value of \( \{r_1, \dots, r_k\}  \). 
	\item Then, we output \( t / \alpha \).
	\item The reason we want to use the \( t \)-th smallest is because want our algorithm to be tolerant to 
		outliers. 

		\question{Why not just use the \( N / 2 \)-th smallest? Shouldn't this be the most tolerant value 
		to outliers?}

		\answer{It is, you can repeat the same analysis for exceedingly large values.} 
\end{itemize}
\subsubsection{Analysis}
\begin{itemize}
	\item We have the property that (based on our algorithm):
		\[
		\mathrm{Pr}[\text{alg outputs} \ge  2k] = \mathrm{Pr}\left[\alpha \le  \frac{t}{2k}\right] = 
		\mathrm{Pr}\left[ \sum_i C(i) \ge  t \right] 
		\] 

		\question{How do we go from the 2nd to the 3rd step?}

		\answer{\( C(i) \) is defined to be the number of outliers, so the probability that we output 
		something larger than \( 2k \) is if there are more than \( t \) outliers.} 

		Here, we define \( C(i)  \) as follows:
		\[
		C(i) = \begin{cases}
			1 & r_i \le  \frac{t}{2k}\\
			0 & \text{otherwise}
		\end{cases}
		\] 
		In other words, \( C(i) \) is an indicator that counts the number of outliers. Then, we have:
		\[
			E\left[ \sum_{i} C(i) \right]  = \sum_{i} E[C(i)] = \sum_i \mathrm{Pr}\left[ r_i\le \frac{t}{2k} \right] = \sum_i \frac{t}{2k} = \frac{t}{2}
		\] 
		Where the second step is reached via linearity of expectation. This means that the expected 
		number of outliers is \( \frac{t}{2} \).

		\question{Is this a good value?}
	\item Now we compute the variance to get a good idea of the spread of \( \sum_i C(i) \):
		\[
			\Var\left[\sum_i C(i)\right] = \sum_i \Var[C(i)] 
		\] 
		Since each \( C(i) \) is associated with an \( r_i \) which is derived from \( h(s_i) \), each 
		\( C(i) \) is also pairwise independent. This allows us to take the summation out of the variance. Then:
		\[
			\Var[C(i)] \le  E[C(i)^2] = E[C(i)] = \frac{t}{2k} \implies \Var\left[ \sum_i C(i) \right] \le k 
			\cdot \frac{t}{2k} = \frac{t}{2}
		\] 
\end{itemize}
