\documentclass[10pt]{article}
\usepackage{../../local}
\usepackage{listings}
\lstdefinestyle{tt}{basicstyle=\small\ttfamily,keywordstyle=\bfseries,language=[LaTeX]{TeX}}



\newcommand{\classcode}{CS 170}
\newcommand{\classname}{Efficient Algorithms and Intractable Problems}
\renewcommand{\maketitle}{%
\hrule height4pt
\large{Eric Du \hfill \classcode}
\newline
\large{Notes} \Large{\hfill \classname \hfill} \large{\today}
\hrule height4pt \vskip .7em
\normalsize
}
\linespread{1.1}

\newcommand{\pre}{\mathrm{pre}}
\newcommand{\prev}{\mathrm{prev}}
\newcommand{\post}{\mathrm{post}}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\cost}{\mathrm{cost}}
\newcommand{\size}{\mathrm{size}}
\newcommand{\capacity}{\mathrm{capacity}}

\newcommand{\question}[1]{\textcolor{red}{#1}}
\newcommand{\answer}[1]{\textcolor{green!80!black!}{#1}}
\renewcommand{\comment}[1]{\textcolor{blue!50}{#1}}
\begin{document}
	\maketitle
	\section{Graphs}
	Formal definition of undirected graphs:
	\begin{itemize}
		\item Called $G(V, E)$.
		\item has a set $V$ of vertices, and $E$ of edges.
		\item Edges are denoted as a pair of vertices $\{v_i, v_j\}$, and undirected mean that each 
			pair of $\{v_i, v_j\}$ is unique.
			\begin{itemize}
				\item e.g. Facebook friendship graphs, where the friendship must be mutual. (Has nearly 3 billion
					nodes, and each node has 330 vertices)
			\end{itemize}
		\item Directed graph is denoted in the same way, except that $\{v_i, v_j\}$ means 
			specifically that $v_i \to v_j$.
	\end{itemize}

	\subsection{Size of Graphs}
	\begin{itemize}
		\item Generally denoted as the number of vertices (denoted by $n$) and the number of edges 
			(denoted by $m$).
		\item $m$ can be at most $n^2$, in the case that every vertex is connected to every other vertex 
			(in which case we have $n(n-1)$ edges)
		\item Degree: the number of edges that a vertex has. With directed graphs, we can specify in-degrees
			and out-degrees.
	\end{itemize}

	\subsection{Representing Graphs}
	\begin{itemize}
		\item Represented either as an adjacency matrix or an adjacency list.  
			\begin{itemize}
				\item Adjacency matrix: have the vertices listed out on both row and column, put a 1 if 
					$\{v_i, v_j\}$ are connected on our graph.
				\item Adjacency list: List of length $n$, and each cell is a linked list that points 
					to all the neighbours of $v_i$.
			\end{itemize}
		\item There are tradeoffs for both ways:
			\begin{center}
				\begin{tabular}{c|c|c}
			& 	\textbf{Adjacency Matrix} & \textbf{Adjacency List}\\
			\hline
					Storage Size  & $O(n^2)$  & $O(n + m)$\\
				Checking whether $(u, v) \in E$ & $O(1)$ & $O(\deg(u))$\\
				Enumerate all of $u$'s neighbours &  $O(n)$ & $O(\deg(u))$
				\end{tabular}
			\end{center} 
		\item We will work with adjacency lists, since the storage size for adjacency matrices get
			too large too quickly. 
	\end{itemize}
	\subsection{Graph Connectedness}
	\begin{itemize}
		\item We first have to solve the problem of graph traversal. Our approach will use ``string and chalk,''
			so we mark when we've reached a dead end and the ``string'' will allow us to backtrack. 
		\item Algorithm description:

\begin{lstlisting}[style=tt]
def explore():
	visited[u] = true

	For all edges of $u$: if visited[v] = false then run explore(v)
\end{lstlisting}
		\item Explore guarantees that all the vertices that are visited by explore have a path from $u$ to 
			that vertex, and vice versa.
			\begin{itemize}
				\item We prove the other direction: if there's a path, then that node is visited.

					Assume that this is false: assume $\exists v$ that hasn't been explored but there is 
					a path from $u$ to $v$. Instead of looking at $u$ to $v$, we look at the path 
					from $u$ to $v_k$, the first node along the path from $u$ to $v$ that is unexplored. 
					This means that the algorithm reached $v_{k - 1}$, then failed to explore $v$.

					This is a contradiction, since explore must have been called on $v_{k-1}$, but failed to 
					recurse down $v_k$. 
			\end{itemize}
	\end{itemize}

	\subsection{Depth First Search (DFS)}
	Essentially calling explore, except it does it recursively on all the remaining nodes that haven't 
	been visited yet.
	\begin{itemize}
		\item We can also use DFS to find the connected components of a graph! When we explore with DFS, we are 
			implicitly exploring connected components, this uses the transitive fact of the \texttt{explore()}
			function.
		\item The edges that are visited by DFS are categorized into tree edges and back edges. Tree 
			edges are edges that are used when the algorithm runs, and back edges are ones that exist within 
			the original graph but aren't used. 
		\item There's a third class called a \textit{cross edges}, but they cannot exist for an undirected graph.
			\begin{itemize}
				\item The proof is by contradiction: imagine that it did exist, then DFS would have called
					explore on that ancestor; but it can't possibly have since $u$ and $v$ do not 
					exist in the same branch.
				\item They \textit{can} exist in a directed graph, since we only traverse through out 
					edges (so there can be an in-edge that never gets traversed).
			\end{itemize}
	\end{itemize}	

	\subsubsection{DFS Runtime}
	\texttt{Explore()} is called only once per node, and the runtime for each node for explore is 
	proportional to $\deg(u)$, so the total is:
	\[
		\sum_{u \in V} O(1 + \deg(u)) = O(n + m)
	\] 

	\subsection{DFS for Directed Graphs}
	It's the same principle, our explore still only runs recursively on unvisited neighbours, but we need 
	to also keep track of the amount of time at which we start and finish processing that node.
	\begin{itemize}
		\item Every time we enter a node, we stamp it with the time of the start clock. When we come 
			out, we stamp again with the end clock. Every time we progress the clock, we increment it by 1.
		\item The point of the clock will be revealed later on in future lectures. 
		\item The edges we keep track of here are forward, back and cross edges. Forward means we go 
			down the tree from ancestor to descendant (not its immediate child), back means we go backwards 
			(can be immediate) and cross edges are defined exactly as before.
	\end{itemize}
	The edges (pre, post, cross) are useful in determining properties of the graph $G$. 
	\begin{itemize}
		\item Cross edges can only go from a later point in one branch to earlier than the other branch
			and not the other way around, due to the way that DFS executes depth-first.
		\item Suppose $(u, v) \in E$ is a tree edge. Then, we know that $\pre(u) < \pre(v)$ (since $u$ is 
			hit first), and $\post(u) < \post(v)$. 
		\item Suppose that $(u,v)\in E$ is a back edge. Then, we have $\pre(v) < \pre(u) < \post(u) < \post(v)$.
	\end{itemize}

	\section{Strongly Connected Graphs}
	Recall that we saw earlier that DFS basically just calls explore repeatedly, to find all the vertices 
	reachable from a vertex $u$. We also introduced two clocks, where when we first visit a node 
	we stamp it with the start clock, and that vertex will have a $\pre(u)$ quantity that stores its value, 
	then increments clock. When we leave the vertex, we stamp again with a $\post(u)$. 

	\begin{itemize}
		\item For cross edges (the only thing we didn't finish last time), we have $\pre(v) < \post(v) <
			\pre(u) < \post(u)$ (we get to $v$ and finish exploring $v$ before we ever touch $u$).
		\item As a recap:
			\begin{center}
				\begin{tabular}{c|c}
					Edge type & Relation\\
					\hline
					Tree edge & $\pre(u)< \pre(v) < \post(v) < \post(u)$\\
					Back edge & $\pre(v) < \pre(u) < \post(u) < \post(v)$\\
					Cross edge & $\pre(v) < \post(v) < \pre(u) < \post(u)$
				\end{tabular}
			\end{center}
	\end{itemize}


	\subsection{Topological Sort} 
	\begin{itemize}
		\item The process of finding an ordering of vertices so that no edges go backwards. This is used 
			in software package loading, to make sure that things aren't being and that are being downloaded
			in the chronological order. 
		\item Mathematically, if $u$ comes before $v$ in the ordering, then there is no edge $(v, u)$. 
		\item Two types of special nodes: \textbf{sources} and \textbf{sinks}
			\begin{itemize}
				\item \textbf{Source:} A node that has no incoming edges and has only outgoing edges. 
				\item \textbf{Sink:} A node that has no outgoing edges and only has incoming edges
			\end{itemize}
		\item Note that a node with no edge at all is considered both a source and a sink.
	\end{itemize}
	
	\subsection{DAGs}
	\begin{itemize}
		\item Called a directed acyclic graph, or basically just a graph without any directed cycles. If 
			we have a cycle, then we can't toplogically sort. 
		\item We will find that if we run DFS on a graph, it is a DAG iff it has no back edges.
			We prove that if we have a back edge, then it cannot be a DAG: since they go from something 
			that's already been visited to something that's been visited earlier, then we have already 
			found a cycle. 

			Now we prove that if we don't have a DAG (i,e, has a cycle) , then it has a back edge. Since DFS
			visits every vertex in 
			a graph, it will eventually enter our cycle at some $v_i$. Then, it will traverse through the cycle
			until it visits all the nodes and eventually gets to $v_k$, the node right before it comes back to 
			$v_i$. 
			Then, $v_k \to v_i$ will become a back edge, since it was visited earlier in the graph.
		\item Back edges are really special! Recall that we only have a back edge when $\post(u) < \post(v)$, 
			since the other edges have $\post(v) < \post(u)$. We then conclude that if we have a DAG, then 
			it has the property that $\post(v) < \post(u)$ (since it can't have back edges). 
			
			\question{Does 
				the logic work the other way around? Does 
			this mean that if $\post(u) < \post(v)$ then we have a DAG?}

			\answer{Not necessarily. Just because $\post(v) > \post(u)$ doesn't necessarily imply that a 
			back edge exists.}
		\item Our algorithm for topological sort: do a DFS on graph $G$, then enumerate all $v \in V$ in 
			the decreasing order of $\post(v)$. 
	\end{itemize}

	\subsection{Strongly Connected Components}
	
	\begin{itemize}
		\item To find DAGs on undirected graphs we can run DFS on them, but what about directed graphs? 
		\item \textbf{Definition:} Vertices $u$ and $v$ are strongly connected if there is a path from $u$ to 
			$v$ and there is also a path from $v$ to $u$. 
		\item A graph is \textit{strongly connected} when all of its vertices are strongly connected.
		\item Generally, we partition a graph into \textit{strongly connected components}, since it's very rare
			that the 
			entire graph is strongly connected.
		\item Strong connectivity is an example of an equivalence relationship, since it satisfies all 
			the properties: reflexive, symmetric and transitive.
			\begin{itemize}
				\item Every vertex is strongly connected to itself
				\item If $A$ is strongly connected to $B$, then $B$ is strongly connected to $A$. 
				\item If $A$ is strongly connected to $B$ and $B$ is strongly connected to $C$, then $A$ 
					is strongly connected to $C$.
			\end{itemize}
		\item If we flip all the edge (i.e. reverse the direction), the strongly connected components don't 
			change at all! 
		\item We can then divide this into a \textit{meta graph}, where we group the graph by strongly 
			connected components (try to be as general as possible when you do this).
			\begin{itemize}
				\item The meta graph can't have cycles! Had a cycle existed, then it would imply that vertices 
					from one strongly connected component can reach vertices of another, and vice versa!
				\item Therefore, the meta graph \textit{must} be a DAG.
			\end{itemize}
		\item Why care about SCC? It is useful in many different fields, since SCCs naturally imply 
			a strong equivalence of objects in a graph.
	\end{itemize}
	\subsection{Finding SCCs}
	\begin{itemize}
		\item \textbf{Attempt 1:} Consider all possible decompositions and check (BAD!)
		\item \textbf{Attempt 2:} Consider pairs of nodes, then run explore to see if they reach the other. 
			(ALSO BAD!)
		\item There exists an algorithm that runs this in $O(n + m)$ time, where we have to run DFS (smartly) 
			only twice!
	\end{itemize}
	\subsubsection{More Properties of SCCs}
	\begin{itemize}
		\item It actually matters where we start our DFS, since sometimes we can't exit the particular 
			connected component. Specifically, if the node is part of a source in the meta graph, then 
			it will visit many nodes, whereas if the node is a sink then we never exit that particular connected
			component.
		\item The ``right'' place to start the DFS is in the \textbf{sink of the meta graph!} The key 
			thing is that we shouldn't exit that connected component, so running \texttt{explore()} on 
			any vertex within this CC will give us the whole SCC.
		\item \textbf{Idea:} Do the topological sort on the meta graph, then run DFS on the sinks of 
			the meta graph.
		\item Suppose we run DFS on a graph $G$. Let $C$ and $C'$ be two connected components such that $C \to 
			C'$ in the meta graph. Then, $\max(\post(v \in C)) > \max(\post(u \in C'))$.

			\textit{Proof:} Split into cases, based on where in the graph we started:

			Case 1: Suppose DFS visited $C$ first ($u \in C$ is the first node visited by DFS. Then, 
			DFS will explore $C'$ first before finishing its exploration of $C$. This means that DFS finishes
			$C'$ before finishing $C$, meaning that $\max(\post(v \in C)) > \max(\post(u \in C'))$.

			Case 2: Suppose DFS visited $C'$ first. Then, we will never visit $C$ since there is no directed
			edge between $C' \to C$. Therefore, the post of $C'$ stops before DFS even reaches $C$, Hence, 
			we have $\max(\post(u \in C')) < \max(\post(v \in C))$. 
		\item Immediate corollary of this: Suppose we ran DFS on a graph $G$. The highest $\post(v)$ belongs 
			to a node $v$ that is in the source SCC of the meta graph!
			\begin{itemize}
				\item Imagine $\max(\post(C))$ is not the largest $\post(v)$ value. Then, this means that 
					there exists another $C''$ whose $\post(C'')$ is larger than that of $C$!
			\end{itemize}
		\item So now we have a way of finding the source of the vertices in $C$, but we want to start from 
			sinks, so we \textbf{flip the edges, then run DFS!} The only difference betweeh $G$ and $G^R$ (the 
			reversed graph) is the direction of the edges; the connected components remain the same.

			\question{Instead of flipping edges, why not run the algorithm from the minimum post value 
			instead?}

			\answer{Because the minimum isn't guaranteed to be a SCC in the same way the max is.}
	\end{itemize}

	\subsection{The Algorithm}
	\begin{itemize}
		\item Compute $G^R$.
		\item Run DFS on $G^R$.
		\item Store the post numbers of this DFS in array called post-r 
		\item Run DFS on $G$, but explore unvisited nodes in \textit{decreasing} order of post-r. For every 
			DFS we run, we find a new SCC.
	\end{itemize}
	\section{Paths in Graphs}

	\subsection{Single Source Shortest Path (SSSP)}
	\begin{itemize}
		\item We want to compute the distances from a source $s \in V$ to other nodes in $V$. 
		\item We don't use DFS here because DFS might explore much longer paths first, so it might be very 
			inefficient.
		\item Solution: use \textbf{Breadth-First Search (BFS)}
			\begin{itemize}
				\item Analogous to a bird's eye perspective, where we explore successively outward in 
					``neighbourhoods.''
				\item Start at exploring from distance 1, then when everything at distance 1 is explored, 
					continue to explore at distance 2, etc. 
			\end{itemize}
		\item The type of BFS that we use depends a lot on what kind of graph we're dealing with:
			\begin{itemize}
				\item Unweighted graphs: Ordinary BFS works
				\item Positive Weights: Dijkstra's algorithm
				\item Negative Weights allowed: Bellman-Ford Algorithm
			\end{itemize}
		\item Going down this list makes the graph more general, but they are less efficient than the ones 
			above. 

			\question{Would a more correct statement be that BFS works if all the edges have the same weight?}
	\end{itemize}

	\subsection{Breadth First Search (BFS)}
	\begin{itemize}
		\item Start at $s$, and add all the neighbours of $s$ to a queue. For every vertex in 
			the queue, we visit all the unvisited nodes from that vertex, and add it to the queue. Repeat
			until all nodes have been visited.
	\end{itemize}

	\subsubsection{Runtime of BFS}
	\begin{itemize}
		\item 	We enqueue and deque every node exactly once if the node is connected, otherwise we don't do it 
			at all. This takes $O(1)$ time. 
		\item Once an item is dequeued, we need to check all the neighbours of a graph, costing $O(\deg(u))$ 
			time.
		\item In total, our runtime is:
			\[
				\sum_{u \in V} O(1 + \deg(u)) = O(n + m)
			\] 
			This is the same runtime as DFS, which is not a coincidence! DFS and BFS are actually related, 
			except the queue is replaced by a stack.
		\item We didn't implement it as a stack in lecture, but the idea is the same.
	\end{itemize}

	\subsection{Weighted Graphs}
	\begin{itemize}
		\item BFS doesn't work here because it ignores the weights of the graph. It is possible that a graph 
			ends up being shorter but goes through more nodes, a possibility that BFS doesn't catch.
		\item \textbf{Useful Fact:} Any sub path of a shortest path is also a shortest path. This is rather 
			obvious.
		\item So what we should think about is that to build the shortest path, we build the shortest path from 
			other, shotest paths but add in the shortest edge. This guarantees that our shortest path 
			remains the shortest.
	\end{itemize}

	\subsection{Dijkstra's Algorithm}
	\begin{itemize}
		\item Let $K$ denote the set of ``known'' nodes where the length of shortest path is computed. To 
			determine node we should add to $K$, we should select the vertex that gives the smallest 
			$\dist(s, u) + \ell(u, v)$. Visually:
			\begin{center}
				\includegraphics[scale=0.5]{dijkstra.png}
			\end{center}
		\item The red region is the set of nodes that we look at.
		\item We don't need to recompute all distances at every iteration - instead we can just store 
			the distances as we go along. Initial overestimates are fine, since eventually we will explore 
			the shortest path, and its distance will eventually be updated. 
		\item If we find a shorter path later on, we can update $\dist(s, u)$ to reflect that.
		\item If we want to find the shortest path from $S$, then we can add a new variable that stores 
			the previous node in the sequence from $S$ to $u$. Therefore, when we want to find the 
			shortest path, then we are continually looking backward until we get back to $S$. 
	\end{itemize}

	\subsubsection{Runtime of Dijkstra's}
	\begin{itemize}
		\item The runtime of Dijkstra's depends on the kind of data structure we used to keep track 
			of the distances:
			\begin{center}
				\begin{tabular}{c|c|c|c|c}
					\textbf{Implementation} & \textbf{Insert} & \textbf{Delete Min} & 
					\textbf{Decrease Key} & \textbf{Runtime}\\
					\hline 
					Array & $O(1)$ & $O(n)$ & $O(1)$ & $O(n^2 + m) = O(n^2)$\\
					Binary Heap & $O(\log n)$ & $O(\log n)$ & $O(\log n)$ & $O((n + m) \log n)$\\
					Fibonacci Heap & $O(1)$ & $O(\log n)$ & $O(1)$ & $O(n \log n + m)$
				\end{tabular}
			\end{center}
		\item The best known runtime of Dijkstra's algorithm is $O(n \log \log n + m)$.
		\item At the end of the day, this is slower than DFS, by the $\log n$ term.
	\end{itemize}
	
	\subsection{Negative Weights: Bellman-Ford Algorithm}
	\begin{itemize}
		\item Sometimes, having negative weights is possible, for instance when traversing an edge is more 
			beneficial to you in some way.
		\item Shortest paths don't really make sense if a cycle has negative length (since then we'd be 
			infinitely descending) 
		\item All we need to do is modify Dijkstra's update function!
			\begin{itemize}
				\item Call an update ``safe'' if $\dist(w)$ is an overestimate of the true shortest path 
					between $s$ and $w$. In other words, $\dist(w) \ge d(s, w)$ for all $w \in V$. 
			\end{itemize}
	\end{itemize}

	\question{see lectures for Bellman-Ford}

	\section{Greedy Algorithms}

	\begin{itemize}
		\item Are algorithms that build their solution piece by piece, and always takes the piece that 
			offers the \textbf{most obvious and immediate benefit}.
		\item Some applications of where Greedy algorithms do work: Scheduling, satisfiability, Huffman Coding
			MSTs. 
	\end{itemize}
	\subsection{Scheduling}
	\begin{itemize}
		\item Input: collection of jobs specified by their time intervals $[s_1, e_1], \dots, [s_n, e_n]$. We
			want to find the largest subset of jobs that have no time conflicts.
		\item To do this, after choosing an interval, we'd want to choose the next interval that 
			has the \textit{earliest end time}. Jobs that finish earlier give us more opportunities to 
			slot in more jobs later in the day. 
			\begin{itemize}
				\item This is not achieved by selecting the shortest job, because it does not give us freedom 
					in where $s_i$ and $e_i$ are. 
				\item This is also not achieved by selecting the earliest job, since we don't know where $e_i$
					is. 
			\end{itemize}
		\item So our code is as follows:

			While set of intervals is not empty: \\
			Add interval $j$ with the earliest finish time $e_j$. \\
			Remove any conflicted interval $i$ from the set, i.e. $[s_j, e_j] \cap [s_i, e_i] \neq 
				\emptyset$.
		\item The runtime of this algorithm is $O(n)$ if the intervals are already sorted by the end time,
				otherwise, we'd need $O(n \log n)$ time since we'd need to sort the intervals first.
		\item To show that the greedy algorithm works, we need to show that this algorithm doesn't rule out 
			an optimal solution.
		\item Induction is very nice to prove these algorithms, since we'd just need to prove that 
			the algorithm selects optimally at every time step! Let's try this for our scheduler: 

			Claim: For any $m \le k$ there is an optimal schedule OPT that agrees with the Greedy solution 
			$G$ on the first $m$ intervals. More formally, if the OPT can be labelled as a list of $i_1, \dots
			i_m$ and $G$ has a list of $j_1, \dots, j_m$, then we require that $i_1 = j_1, \dots, i_m = j_m$.

			Base case: $m = 0$, this is fairly trivial. Any two schedules agree on 0 things.

			Inductive Hypothesis: This claim holds true for $m$. Now we show $m+1$. 

			Inductive Step: There are two cases we need to consider:
			\begin{itemize}
				\item Case 1: when $i_{m+1} = j_{m+1}$, in which case we are done.
				\item Case 2: $i_{m+1} \neq j_{m+1}$. Then let's define another schedule OPT' which is 
					the same as $OPT$ except for the fact that $i_{m+1}$ is replaced with $j_{m+1}$. 

					Note that $j_{m+1}$ does not conflict with $j_1, \dots, j_m$, since the greedy algorithm does
					not produce time conflicts. 
					Also, $j_{m+1}$ does not conflict with 
					$i_{m+2}$ since $j_{m+1}$ ends earlier than $i_{m+1}$ (by the greedy algorithm). Hence, 
					placing $j_{m+1}$ into this algorithm instead of $i_{m+1}$ produces an \textit{equally 
					valid solution} for the schedule, since the size of OPT' is the same as that of OPT. 
					Therefore, OPT' is also optimal, completing the proof.
			\end{itemize}
			\question{Does this proof by induction assume that the Greedy solution gives a correct 
			schedule?}
		\item In essence, the proof is showing that there is no choice that the Greedy algorithm makes which 
			rules out an optimal solution. 
	\end{itemize}
	\subsection{Horn Formulas}
	\begin{itemize}
		\item Variables $x_1, \dots, x_n$ are either true or false.  
		\item Clauses: 
			\begin{itemize}
				\item ``Implication clause'' where $(x_i \land x_j \land \dots) \implies x_k$. This is 
					equivalent to $\overline x_i \lor \overline x_j \lor \dots \lor x_k$.
				\item ``Pure Negative Clauses'' where $(\overline x_i \lor \overline x_j \lor \dots )$
			\end{itemize}
		\item A Horn formula is an AND of all Horn clauses, which are either implication or pure negative.
		\item There is a problem called Horn-SAT which asks us to find an assignment of variables that makes
			all Horn formulas to be true, if an assignment exists.
		\item Greedy Algorithm: 
			\begin{itemize}
				\item For all $i$, set $x_i$ to be false. 
				\item While there exists an implication $(x_i \land \dots \land x_j) \implies x_k$ being 
					set to false, set $x_k$ to be true. 
				\item If every pure negative clause $(\overline x_i \lor \dots \lor \overline x_j)$ is 
					set to true, we return $(x_1, \dots, x_n)$. 
				\item Otherwise, return ``not satisfiable.''
			\end{itemize}
	\end{itemize}
	\subsubsection{Proof of Correctness}
	\begin{itemize}
		\item We want to show that whenever the greedy algorithm sets a variable $x_i$ to true, it does not 
			ruin a satisfying assignment. In other words, whenever a satsifying assignment exists, 
			then Greedy will output one.
		\item We can show a stronger statement: the set of variables set to True by the greedy algorithm 
			has to be set to true in any other assignment. We prove this by induction 

			Base case: In the 0th iteration of the while loop, nothing is set to true, so we're fine.

			Inductive Hypothesis: The first $m$ variables set to true by Greedy are also true in every satisfying
			solution.

			Inductive Step: Let $x_{m+1}$ be the next variable set to True by the greedy algorithm.
			This means that there was an unsatisfied implication $(x_i \land \dots \land x_j) \implies x_k$ 
			where the LHS was true, and $x_{m+1}$ is false. This only happens when $x_i, \dots x_j$ are all 
			set to True, by the greedy algorithm on $m$ steps (which we know to match the optimal solution by 
			inductive hypothesis).

			Then the only way to satisfy this condition MUST have $x_{m+1}$ set to true as well, and that 
			completes the proof.
		\item Now we have to prove correctness. If Greedy outputs a solution, then it must be satisfiable --
			this is fairly obvious, since the while loop and if condition makes sure that all clauses are 
			satisfied. 
		\item We also want to 
	\end{itemize}
	\subsection{Codes}
	\begin{itemize}
		\item Usually (things like ASCII) encode English characters using a fixed length of bits per character.
		\item If our goal is to save space, then we probably don't want that. Particularly, there are letters
			 that appear more often than other characters, so if we were to use the same space for every 
			 character that'd be fairly wasteful.
		 \item Assume that we have four letters with varying frequencies:
			 \begin{center}
				 \begin{tabular}{cc|c|c|c}
					 \multicolumn{1}{c|}{\textbf{Frequency}} & \textbf{Letter} & \textbf{Encoding 1} &
					 \textbf{Encoding 2} & 
					\textbf{ Encoding 3}\\
					 \multicolumn{1}{c|}{0.4} & A & 00 & 0 & 0 \\
					 \multicolumn{1}{c|}{0.2} & B & 01 & 00 & 110 \\
					 \multicolumn{1}{c|}{0.3} & C & 10 & 1 &10 \\
					 \multicolumn{1}{c|}{0.4} & D & 11 & 01 &111 \\
					\hline 
					   & & & \\
					 Total Cost:  & & $2N$ & $\begin{aligned} N(0.4 + 0.3) &+ 2N(0.1 + 0.2)\\& = 1.3N
						 \end{aligned} $& $\begin{aligned} 0.4N + 2N(0.3) & + 3N(0.2 + 0.4)\\
										&= 1.9N \end{aligned}$
			 	\end{tabular}
			 \end{center}
		 \item There are issues with encoding 2: it's lossy in the sense that AB is encoded in the same way that 
			 BA is coded.
		 \item These issues are solved in encoding 3, and we found that we can still do better than the $2N$ 
			 from our naive application where every letter gets the same number of bits.
	\end{itemize}

	\section{Huffman Coding, MSTs}
	Recap of Greedy algorithms:
	\begin{itemize}
		\item Our goal is to prove that whenever a choice is made, that an optimal solution still exists, proven
			to exist via induction.
		\item Base case: at the beginning, achieving optimal choice is always possible
		\item Inductive Hypothesis is the same as normal induction, and inductive step is to show that if 
			an alternate choice is made it doesn't violate the optimal solution.
	\end{itemize}
	\subsection{Prefix codes and Trees}
	\begin{itemize}
		\item Prefix codes can be represented as a binary tree with $k$ leaves. 
		\item the code is the ``address'' of a letter in the tree (i.e. the string of numbers leading from 
			the root to that leaf). We want to order the tree from highest to lowest frequency, so that the 
			letters with the highest frequency uses less characters.
		\item In general, the cost for such a tree is:
			\[
				\mathrm{cost} = \sum_{i = 1}^n f_i \cdot \text{depth(leaf $i$)}
			\] 
		\item Our goal is to find an \textit{optimal subtree}. What does such a tree look like? 

		Answer: Even if 
			we don't know what the frequencies are, the optimal code should be a \textbf{full binary tree.} 

		\item Now
			we need to prove that there exists an optimal tree when the
			two lowest frequency symbols are siblings 
			of each other.

			\textit{Proof:} By contradiction, let $x, y$ be symbols with lowest frequencies and assume they 
			aren't siblings. Let $a, b$ be the deepest pair of siblings. Since $x, y$ aren't siblings 
			of each other, then only one of $a, b$ are one of $x$ or $y$. WLOG, let $x = a$. 
			What happens if we swap $x,y$ and $a, b$? Well, we know that $f_a, f_b \ge f_x, f_y$, and we've 
			reduced the length of $a, b$ while also reduced frequency of the deepest entries in the tree, meaning
			that we've ended up with a cheaper tree! Hence, the original tree could not have 
			been an optimal tree. 
	\end{itemize}
	\subsection{Algorithm (Huffman Coding)} 
	See below for pseudocode:
	\begin{center}
		\includegraphics[scale=0.5]{Huffman.png}
	\end{center}
	\begin{itemize}
		\item The idea is to recursively generate a tree using the lowest frequencies, and combining them 
			together by adding the frequencies of each tree's children. 
		\item \textbf{Runtime Analysis:} Storing in our priority queue can be optimized by using a binary heap, 
			which takes $O(n \log n)$ time. Combination also takes $O(n \log n)$ time, so total runtime
			 is $O(n \log n)$.
			\begin{itemize}
				\item Inserting into priority queue takes $O(n \log n)$ time. 
				\item At every step of the while loop, we perform 2 deleteMin instructions, which is constant
					time.
				\item There is 1 insert each time (after the connection). This means that on every 
					iteration, we are halving the number of nodes (hence $O(n \log n)$). 
				\item So total time complexity is $2O(n \log n) = O(n \log n)$.
			\end{itemize}
		 \item This generates a full binary tree with optimal coding. 

			 \textit{Proof:} We show that a greedy selection (which is what Huffman Coding is doing) does not 
			 rule out an optimality coding.

			Base case: $n = 2$. We can generate optimal code using $0$ for first letter and $1$ for 
			second letter. Huffman coding does the same.

			Inductive Hypothesis: Assume that this works for $n-1$ letters.

			Inductive Step: Let $T$ be the optimal tree for the frequencies $f_1, \dots, f_n$. WLOG, let 
			$f_1 \le \dots \le f_n$. Assume that the two lowest frequency codes are siblings (proven 
			from earlier), and merge the two into a single node, where $f = f_1 + f_2$. Looking at the cost 
			of the new tree $T'$, we know that 
			\[
				\mathrm{cost}(T) = \mathrm{cost}(T') + f_1 + f_2
			\] 
			Huffman coding also does this merging process, and our inductive hypothesis guarantees that this 
			tree is optimal on $n-1$ letters, so when we split back into the two characters it is still
			guaranteed to be optimal. Formally, if $H'$ is the cost of the reduced tree, then
			\[
				\mathrm{cost}(H) = \mathrm{cost}(H') + f_1 + f_2
			\] 
			which is the same as the cost relationship with $T$, so Huffman coding does indeed give an optimal 
			coding.
	\end{itemize}
	\question{What if $f_1 + f_2$ happens to be large enough such that $f_1 + f_2 > f_n$?}

	\answer{Apparently it doesn't matter?} 
	\subsection{Minimum Spanning Trees (MSTs)}
	\begin{itemize}
		\item Tree also has edges, so we can assign a cost as well: $\cost(T) = \sum_{e \in T} w_e$ (the 
			sum of the weights).
		\item Suppose we're given a graph $G(V, E)$ with non-negative weights. We want to find a set of edges
			that connects the graph, and has the smallest cost. 
		\item Why do we care? This gives the notion of connectivity in a network, so you can think of cell 
			towers or roads/railways as practical applications.
		\item We will use the same approach: first we ask about what an MST looks like.
			\begin{itemize}
				\item It will be an acyclic graph, since removing an edge that's part of a cycle still preserves
					the connectivity in the graph.
			\end{itemize}
	\end{itemize}
	\subsubsection{Graph Structures and Cuts}
	\begin{itemize}
		\item \textbf{Cuts:} a way to partition a graph that splits up the vertices into two groups. 
		\item They're important because cuts go through edges to divide vertices into groups.  
		\item Imagine we've already discovered some edges $X$ of the MST. Consider the cut that doesn't cut 
			any edges $X$. Now we look at the edges that are being cut. The edge from a larger MST is being cut,
			and it's the lowest weight edge that we should add to our MST!

			\textbf{This is a very important property, it shows that \textit{any} cut that we make 
				is an edge that \textit{can} be added to our MST, so therefore regardless of which vertex
			we start searching at, an MST is still guaranteed.}
		\item Therefore, we should add this edge and its corresponding vertex to our MST.
		\item We can formalize this argument via a proof, but I'm too lazy to write it here.
		\item Turns out that any algorithm that fits the following properties forms an MST:
			\begin{itemize}
				\item Start with $X$, an empty list.  
				\item Pick $S \subseteq V$ such that $X$ has no edges from $S$ to $V \setminus S$
				\item Choose the lighest weight edge from $S$ to $V \setminus S$. 
				\item Add edge to $X$.
			\end{itemize}
		\item The proof for why this does give an MST can be done via induction.
	\end{itemize}
	\subsubsection{Kruskal's Algorithm}
	\begin{itemize}
		\item Instead of doing $S$ and $V \setminus S$, it instead selects edges, and checks whether 
			the edge forms a cycle. If it does, we don't add this edge. This process of checking a cycle 
			actually does split our graph into $S$ and $V \setminus S$, albeit implicitly. 
		\item We show correctness by showing that Kruskal's algorithm fits the meta algorithm given above. 
	\end{itemize}

	\section{A different greedy algorithm for MSTs}
	
	\subsection{Prim's Algorithm}
	\begin{itemize}
		\item The idea is to draw a tree by greedily adding the cheapest edge that can grow the tree.
		\item Start from some vertex, and repeatedly pick the lightest edge $(u, v)$ such that 
			$u \in S$ and $v \in V \setminus S$. 

			\question{How exactly is this different from Kruskal's algorithm? Aren't both using cuts in 
			the same way?}

			\answer{Kruskal's doesn't start from a given vertex, but instead just selects edges. Prim's 
			starts with vertices and looks at edges that connect from $S$ to $V \setminus S$}
		\item Remember: the shape of the MST is dependent on the node that we start at, but an MST will 
			always exist no matter which vertex we start at.  
		\item Both Prim's and Kruskal's algorithm works on negative edge weights. This is because the cut 
			property still holds, and the notion \textit{minimum} spanning tree is not broken with 
			negative edge weights.
	\end{itemize}

	\subsection{Implementation}
	\begin{itemize}
		\item The naive implementation of Prim's is actually quite slow, since on every added vertex we 
			are looking for new cuts and checking edges every time.
		\item We can optimize by using priority queues (basically a max heap based on priority).  

			\question{Is this true about priority queues?}
		\item So here are the things we need to keep track of:
			\begin{itemize}
				\item For every edge $v \in V \setminus S$, check whether $v$ has a direct 
					edge of the set $S$ of ``visited'' vertices, and also the cost of the lightest 
					edge connecting
					$v$ to the set $S$ of visited vertices.
				\item We had the same dilemma before, with Dijkstra's algorithm!
			\end{itemize}
		\item So let's follow the same procedure as Dijkstra's!
			\begin{itemize}
				\item First start with $\dist(v)$ set to infinity, and $\prev(v)$ to null for 
					every vertex.
				\item If a neighbor $u$ is added to $S$ (visited set) and $\dist(v) > w_{u, v}$, then 
					we update $\dist(v) = w_{(u, v)}$, and set $\prev(v) = u$. 
				\item This is slightly different from Dijkstra's, where the dist array instead marks 
					the minimum edge between two visited nodes, instead of the total distance 
					from a certain vertex. 
				\item Part of the reason for this is that MSTs don't care about where you start. 
			\end{itemize}
		\item The ``cut" in this case is actually the process of adding adjacent edges from visited nodes
			to unvisited ones into the priority queue. 
	\end{itemize}
	\subsection{Runtime}
	\begin{itemize}
		\item For a priority queue: we can either use a binary heap ($O(\log n)$ for each operation) or 
			fibonacci heap (a little bit better, since $\log(n)$ for inserts but $O(1)$ for everything else. 
		\item So because of the constant time for Fibonacci heap, it has $O(m + n \log n)$ time, whereas
			a binary heap has $O((m + n) \log n)$.
		\item Comparing both algorithms:
			\begin{itemize}
				\item Kruskal's: $O((m + n) \log n)$
				\item Prim's (with Fibonacci heap) $O(m + n \log n)$.
				\item For sparse graphs (so ones with not many edges), both are equally as good.
				\item For dense graphs, Prim's is much better. 
			\end{itemize}
	\end{itemize}
	\subsection{Set Cover Problem}
	\begin{itemize}
		\item Input: the universe of $n$ elements $U = \{1, \dots, n\}$, and subsets 
			$S_1, S_2, \dots, S_m \subseteq U$, such that $\bigcup_{i = 1}^m S_i = U$. 
		\item Output: A collection of $S_i$ of minimal size.
		\item This is an example of a problem where the greedy algorithm is \textit{not optimal!} Instead, 
			it is approximately optimal. 
		\item Claim: if the optimal solution uses $k$ sets, then the greedy algorithm uses 
			at most $k \ln n$ sets. 
		\item We will prove this recursively: let $n_t$ be the number of elements not covered by the greedy 
			algorithm after $t$ choices. Then, we can reframe the problem to be that 
			when $t = k \ln n$, we want $n_t < 1$. 
			\begin{itemize}
				\item Subclaim 1: $n_1 \le n_0 - \frac{n_0}{k}$. 

					\textit{Proof:} the optimal solution requires $k$ sets to cover $U$, so the 
					average number of elements in any set is $\frac{n}{k}$. Hence, there is a set 
					that counts more than $\frac{n}{k}$ elements (if not equal).
				\item Subclaim 2: for any $t$, $n_{t+1} \le n_t(1 - 1 / k)$.
					
					\textit{Proof:} This is a natural extension of claim 1.  
			\end{itemize}
		\item With this proven, we introduce an \textbf{approximation factor}, which is a way to say that Greedy
			is optimal, with an approximation factor of $\ln n$. 
	\end{itemize}

	\section{Dynammic Programming I}

	\subsection{Fibonacci Numbers, revisited}
	\begin{itemize}
		\item Imagine computing Fibonacci numbers; there's a lot of repeated calculations! For instance, 
			$F(1)$ is computed $2^n$ times when we're looking for $F(n)$!
		\item To optimize this, store each successive computation of $F(n)$ into an array that we access, 
			so that we only need to compute each $F(k)$ exactly once.
		\item This is called \textbf{memoization}, where we store things in a ``memo,'' to be accessed by our 
			algorithm later on. 
	\end{itemize}
	\subsection{Elements of Dynamic Programming}
	\begin{itemize}
		\item There are a couple hallmarks of DP:
			\begin{enumerate}
				\item Subproblems, or ``optimal substructure''. Refers to the fact that large problems 
					can be broken up into smaller subproblems. For Fibonacci, this means that 
					$F(n)$ is recursively expressed in terms of smaller subproblems.
				\item Overlapping subproblems: A lot of subproblems overlap with one another. We recurse to 
					smaller subproblems, and in doing so we see that a lot of computation 
					is repeated. The solution to this is to use memoization, so that 
					each computation is done only once.
		\item There are two ways to do DP:
			\begin{itemize}
				\item Top-Down: start from the largest subproblem and recurse to smaller subproblems. 
					This often involves recursion.    
				\item Bottom-up: start from the smallest subproblems then work to larger subproblems.
					Memoization still happens; we just fill the table from the small to 
					largest problems. In this method, this doesn't need a recursive call. 
			\end{itemize}
		\end{enumerate}
		\item The mathematical runtime of top-down and bottom-up are the same.
		\item The computation structure for DP actually looks awfully similar to a DAG. 
		\item If we view every subproblem as a node in the graph: construct it in such a way that 
			an edge $i \to j$ exists if the solution to subproblem $j$ directly depends on the solution to of 
			subproblem $i$. 
		\item Consider a topological sort on this DAG: then the bottom-up solution directly follows the 
			conputation of this DAG! 
			\begin{itemize}
				\item In the top-down framework, we are filling up the memo table in topological sort order, 
					since that table is still being filled from bottom up.
			\end{itemize}
	\end{itemize}
	\subsection{Shortest Paths on DAGs}
	\begin{itemize}
		\item We're given a DAG with a source $s$. We want to find the cost of the shortest path from 
			$s$ to $u$ for all $u \in V$. We also want to do this in linear time, 
			$O(n + m)$.
		\item We can always run a topological sort on this DAG in $O(n + m)$ time. Our subproblems 
			are the distances from $s$ to $u$ for every node $u$.
		\item After ordering in topological sort, we can just go down this graph \textit{in topological 
			order!} This means that the structure of the DP tree is the same as that of the topological sort.
		\item In terms of our recurrence relation, $\dist(u) = \dist(v) + \ell(u, v)$. Here, 
			$\dist(v)$ is implied to be memoized, since it's already a solved problem.
		\item This is an $O(n +m)$ solution to this problem!
	\end{itemize}
	\subsection{DP Recipe}
	\begin{enumerate}[label=\alph*)]
		\item Identify the subproblems (i.e. find the optimal substructure)
		\item Find a recursive formulation for the subproblems: just try to solve it via recursion and 
			see where it gets you.
		\item Design the DP algorithm -- fill in a table, starting with the smallest sub-problems and 
			building up. 
	\end{enumerate}
	\subsection{Shortest Paths with $k$}
	\begin{itemize}
		\item Here we consider the same problem of finding shortest path, but we're restricted to use at most 
			$k$ edges. 

			\question{Fill this out from lecture recordings}
	\end{itemize}
	\subsection{All-Pair Shortest Paths}
	\begin{itemize}
		\item Here, instead of finding the shortest path from a singular source node, we want to find 
			it for all pairs of nodes. 
		\item Input: again a graph with no negative cycles.
		\item Naively, we can run Bellman-Ford on all nodes, but this would take $O(nm)$ a total 
			of $n^2$ times, so our total runtime could be as large as $O(n^4)$ for dense graphs.
			Therefore, we're looking for a better algorithm.
		\item Identify the subproblem: subproblem $k$: for all pairs, find the shortest $u \to v$ path 
			whose internal vertices (so the path they take) only use nodes $\{1, 2, \dots, k\}$.
			\begin{itemize}
				\item In other words, there's a collection of $k$ nodes, and the path from $u \to v$ 
					\textit{only} uses these nodes.
			\end{itemize}
		\item Recursion: When we have the set from $\{1, \dots, k\}$, we want to find the relation between 
			this set and how to expand this set. There are a couple ways that the new 
			node can be added:
			\begin{itemize}
				\item Case 1: The new node added does not lie on the path: then nothing really changes, so 
					$\dist_{k +1}(u, v) = \dist_k(u, v)$.
				\item Case 2: The shortest path uses the added node: this path can be broken into two 
					parts: the shortest $u \to (k+1)$ path and then the shortest $(k+1) \to v$ path. 
					Both of these paths are already computed (by definition of them only using 
					the set $\{1, \dots, k\} $), so we just have to add these two up. 
				\item To combine these two, we find the minimum of these two to find whether the 
					path from $u$ to $v$ has changed or not.  
			\end{itemize}
		\item Runtime: Each update is $O(1)$ time, and we have to loop over $u, v$ a total of $k$ times, 
			so overall $O(n^3)$ runtime. 
		\item This is called the \textbf{Floyd-Washall Algorithm.}
	\end{itemize}
	\input{lec-13.tex}
	\input{lec-14.tex}
	\input{lec-15.tex}
	\input{lec-16.tex}
	\input{lec-17.tex}
	\input{lec-18.tex}
	\input{lec-19.tex}
	\input{lec-20.tex}
\end{document}
